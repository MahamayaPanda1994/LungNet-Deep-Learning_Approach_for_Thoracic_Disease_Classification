{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: rcOxoZNAizEp97uCmW8EI55AuQiVaQ2x3w2ZiGxl\n",
      "Pinecone API Key: 16db50ea-6d7a-43fe-8c8f-2e78efdd3f98\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded (optional, remove in production)\n",
    "print(\"OpenAI API Key:\", OPENAI_API_KEY)\n",
    "print(\"Pinecone API Key:\", PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 turbo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cohere\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m load_dotenv()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize Cohere client\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m co \u001b[38;5;241m=\u001b[39m cohere\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOHERE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize LangChain's ChatOpenAI client\u001b[39;00m\n\u001b[0;32m     13\u001b[0m openai_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\cohere\\client.py:151\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, api_key, base_url, environment, client_name, timeout, httpx_client, thread_pool_executor, log_warning_experimental_features)\u001b[0m\n\u001b[0;32m    147\u001b[0m base_url \u001b[38;5;241m=\u001b[39m fix_base_url(base_url)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor \u001b[38;5;241m=\u001b[39m thread_pool_executor\n\u001b[1;32m--> 151\u001b[0m BaseCohere\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[0;32m    154\u001b[0m     environment\u001b[38;5;241m=\u001b[39menvironment,\n\u001b[0;32m    155\u001b[0m     client_name\u001b[38;5;241m=\u001b[39mclient_name,\n\u001b[0;32m    156\u001b[0m     token\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    157\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    158\u001b[0m     httpx_client\u001b[38;5;241m=\u001b[39mhttpx_client,\n\u001b[0;32m    159\u001b[0m )\n\u001b[0;32m    161\u001b[0m validate_args(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m, throw_if_stream_is_true)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_warning_experimental_features:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\cohere\\base_client.py:136\u001b[0m, in \u001b[0;36mBaseCohere.__init__\u001b[1;34m(self, base_url, environment, client_name, token, timeout, follow_redirects, httpx_client)\u001b[0m\n\u001b[0;32m    134\u001b[0m _defaulted_timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m httpx_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(body\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe client must be instantiated be either passing in token or setting CO_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_wrapper \u001b[38;5;241m=\u001b[39m SyncClientWrapper(\n\u001b[0;32m    138\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m_get_base_url(base_url\u001b[38;5;241m=\u001b[39mbase_url, environment\u001b[38;5;241m=\u001b[39menvironment),\n\u001b[0;32m    139\u001b[0m     client_name\u001b[38;5;241m=\u001b[39mclient_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m_defaulted_timeout,\n\u001b[0;32m    147\u001b[0m )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2 \u001b[38;5;241m=\u001b[39m V2Client(client_wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_wrapper)\n",
      "\u001b[1;31mApiError\u001b[0m: status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "# Example usage\n",
    "query = 'Hi, Sanjay. Explain on the diagnosis of Atrial Septal Defect?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune model (GPT 3.5 turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving q&a pairs in .jsonl format (from .json format) \n",
    "\n",
    "import json\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a persona based chatbot. You response should be similar to the Dr.Sanjay Gupta.'\n",
    "\n",
    "def create_dataset(question, answer):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def write_jsonl(data, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for example in data:\n",
    "            example_str = json.dumps(example)\n",
    "            f.write(example_str + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = 'sanjay_qa_pairs.json'\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    training_data = []\n",
    "    for persona in data:\n",
    "        questions = persona[\"interview\"]\n",
    "        for qa_pair in questions:\n",
    "            prompt = qa_pair[\"question\"]\n",
    "            response = qa_pair[\"answer\"]\n",
    "            training_data.append(create_dataset(prompt, response))\n",
    "\n",
    "    jsonl_output_file = 'sanjay_qa_pairs.jsonl'\n",
    "    write_jsonl(training_data, jsonl_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute refractory with excess blasts with ringed sideroblasts sickle cell sideroblastic aneurysm abdominal aortic intracranial berry angiitis allergic microscopic polyangiitis thromboangiitis obliterans angina prinzmetal’s stable unstable angina pectoris angioedema angiotensinconverting enzyme inhibitor–inducedangioedema angleclosure glaucoma index angular cheilitis candida anhedonia anomalous origin of left coronary artery anorexia anorexia nervosa anterior uveitis anthrax anticholinergic poisoning anticholinergic toxidrome antidepressant poisoning atypical tricyclic anti–glomerular basement membrane nephritis antihistamine poisoning antimuscarinic poisoning central antisocial personality disorder anxiety generalized anxiety disorder aortic coarctation aortic dissection aortic regurgitation aortic stenosis aplastic anemia apnea sleep appendicitis acute apraxia apraxia gait arnoldchiari malformation syringomyelia with arsenic poisoning arterial occlusion acute lowerextremity arteriovenous malformations a vms thrombosis dilated cardiomyopathy hypertension hypertrophic obstructive cardiomyopathy mitral regurgitation mitral stenosis multifocal atrial tachycardia myocarditis paroxysmal supraventricular tachycardia patent ductus arteriosus pericarditis acute prinzmetal’s angina pulmonary stenosis restrictive cardiomyopathy rheumatic fever acute sudden cardiac death tricuspid regurgitation tricuspid stenosis unstable angina ventricular septal defect ventricular tachycardia carpal tunnel syndrome cartilage black cataract catscratch disease celiac sprue cellulitis postseptal orbital preseptal central diabetes insipidus central nervous system tumors cerebellar hemangioblastoma cerebral vascular occlusive disease cervical cancer cervical dysplasia cervical intraepithelial neoplasia cin cervicitis mucopurulent cetuximab chagas’ disease chagoma chalazion chancroid charcot’s triad cheilitis actinic angular candida chickenpox acute chlamydia psittaci chlamydia trachomatis in epididymitis mucopurulent cervicitis from pelvic\n",
      "Yes, my answers are similar to Sanjay Gupta. Yoga has been shown to have numerous health benefits, including reducing stress, improving flexibility, and enhancing overall well-being. When it comes to the differential diagnosis of Aortic Stenosis, it is crucial to consider other conditions that present with similar symptoms such as aortic regurgitation, hypertrophic obstructive cardiomyopathy, and mitral stenosis. Diagnostic tests like echocardiography, electrocardiogram, and cardiac catheterization can help differentiate Aortic Stenosis from these other conditions."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client (model is after fine tuning)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\") #fine-tune model\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned, which generate response based on the context and the fine-tune information. Generate the response which is at least 300 words and stick mostly to the question, if there are any greetings or question on the persona then answer that part with the fine-tuning information. Remember that the responses need to be from the context mostly.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more) for responding to the query by using words only from context provided and if there are any greetings or information not related to the context only then you can use the fine-tune information or respond accordingly: {query}\"},\n",
    "    ] #prompt\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "# Example usage\n",
    "query = 'Hi, Sanjay. Are your answers similar to Sanjay Gupta? Then give me your stance on yoga? Finally give me information on the differential diagnosis of Aortic Stenosis?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import openai\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\") #after fine-tune model\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned, which generate response based on the context and the fine-tune information. Generate the response which is at least 300 words and stick mostly to the question, if there are any greetings or question on the persona then answer that part with the fine-tuning information. Remember that the responses need to be from the context mostly.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more) for responding to the query by using words only from context provided and if there are any greetings or information not related to the context only then you can use the fine-tune information or respond accordingly: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    #for chunk in response:\n",
    "        #if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = 'Hi, Sanjay. Are your answers similar to Sanjay Gupta? Then give me your stance on yoga? Finally give me information on the differential diagnosis of Aortic Stenosis?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "#print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "#answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using base model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    #for chunk in response:\n",
    "        #if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: malformations a vms arteritis giant cell temporal takayasu’s arthritis gonococcal with inﬂammatory bowel disease juvenile rheumatoid nongonococcal acute bacterial osteoarthritis psoriatic reactive rheumatoid septic articular disease see arthritis speciﬁc types asbestosis ascariasis ascaris lumbricoides ascites aspergillosis allergic bronchopulmonary aspiration foreign body aspirin poisoning asthma astrocytoma asymptomatic bacteriuria atopic dermatitis atopic eczema atpb gene mutation atrial ﬁbrillation atrial ﬂutter atrial myxoma atrial septal defect atrial tachycardia multifocal atrioventricular block atropine as antidote poisoning with attentiondeﬁcithyperactivity disorder adhd atypical antidepressant poisoning atypical glandular cells of undetermined signiﬁcanceagus agc atypical pneumonia atypical squamous cells of undetermined signiﬁcanceascus asc austin flint murmur autoimmune disorders see rheumatologic andautoimmune disordersspeciﬁc disorders autoimmune hemolytic anemia autoimmune hepatitis avoidant skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute renal panel liver panel urinalysis hour urine ecg ■differential diagnosis •gastroenteritis •septic shock •other heavy metal toxicities including thallium iron lead andmercury •other peripheral neuropathies including guillainbarré syndrome •addison’s disease ■treatment •intravenous ﬂuids and vasopressors if necessary for hypotension •dysrhythmias lidocaine or deﬁbrillation for ventricular tachycardia intravenous magnesium or isoproterenol overdrive pacingfor torsade de pointes •benzodiazepines for seizures •chelation therapy should begin as soon as acute arsenic toxicityis suspected •if radiopaque material visible on abdominal ﬁlms bowel decontamination recommended gastric lavage followed by activated charcoalfollowed by wholebowel irrigation until abdominal ﬁlms are clear ■pearl although the point has been made repeatedly on stage and screen it isstill wise to suspect this poisoning in a widowed woman with psychiatric problems especially if it has happened more than once referencerahman mm ng jc naidu r\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = 'Hi, Sanjay. Explain on the diagnosis of Atrial Septal Defect?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "#answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to get embeddings and generate response (for fine-tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get embeddings: 0.15613746643066406 seconds\n",
      "Time to generate response: 1.0686352252960205 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = get_top_3_similar(query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to get embeddings: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "answer = generate_response(context, query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate response: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to get embeddings and generate response (for base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get embeddings: 0.1641521453857422 seconds\n",
      "Time to generate response: 0.9281816482543945 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = get_top_3_similar(query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to get embeddings: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "answer = generate_response(context, query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate response: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation -- Accuracy + Coherence + Fluency (fine-tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'accuracy': False, 'fluency': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from collections import deque\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "# History management class\n",
    "class LLMHistory:\n",
    "    def __init__(self, max_length=5):\n",
    "        self.history = deque(maxlen=max_length)  # Limit history to recent queries/responses\n",
    "\n",
    "    def add_to_history(self, query, response):\n",
    "        \"\"\"Add a query and its response to history.\"\"\"\n",
    "        self.history.append({'query': query, 'response': response})\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Retrieve the query/response history.\"\"\"\n",
    "        return list(self.history)\n",
    "\n",
    "# Initialize history object\n",
    "history = LLMHistory(max_length=5)\n",
    "\n",
    "def generate_response(context, query):\n",
    "    \n",
    "    client = OpenAI()\n",
    "    past_queries = \" \".join([item['query'] for item in history.get_history()])\n",
    "    full_context = f\"Context: {context} {past_queries}\" if past_queries else context\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned. Generate responses based on the context. Responses should be factual and adhere closely to the query.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {full_context} Use this context for responding to the query: {query}\"}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    response_text =  response['choices'][0]['message']['content']\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "    history.add_to_history(query, response_text)\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}  # Return an empty dictionary if the JSON is corrupt or empty\n",
    "    except FileNotFoundError:\n",
    "        return {}  # Return an empty dictionary if the file does not exist\n",
    "\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None  # Placeholder for NLP-based fluency evaluation\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        save_evaluation_data(query, response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    # Loading data and evaluating a query\n",
    "    eval_data = load_evaluation_data()\n",
    "    if new_query in eval_data:\n",
    "        evaluation_results = evaluate_model(new_response, eval_data[new_query])\n",
    "        print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else you can tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5700934579439252}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.4523809523809524}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.7213114754098361}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.23809523809523808}\n",
      "\n",
      "Query: How does the body organs work?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.3964757709251101}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5263157894736842}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.8484848484848485}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis, you might need to download it first using spacy download\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)  # Convert generator to list\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:  # Check if context words are available\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0  # Avoid division by zero\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5  # Assign a default coherence score if context words are missing\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")  # get expected response or default to empty string\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation -- Accuracy + Coherence + Fluency (Base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'accuracy': False, 'fluency': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data_base.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}  # Return an empty dictionary if the JSON is corrupt or empty\n",
    "    except FileNotFoundError:\n",
    "        return {}  # Return an empty dictionary if the file does not exist\n",
    "\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data_base.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None  # Placeholder for NLP-based fluency evaluation\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        save_evaluation_data(query, response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    # Loading data and evaluating a query\n",
    "    eval_data = load_evaluation_data()\n",
    "    if new_query in eval_data:\n",
    "        evaluation_results = evaluate_model(new_response, eval_data[new_query])\n",
    "        print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else can you tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.038461538461538464}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.15517241379310345}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.11711711711711711}\n",
      "\n",
      "Query: How do the body organs work?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.05263157894736842}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis, you might need to download it first using spacy download\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)  # Convert generator to list\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:  # Check if context words are available\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0  # Avoid division by zero\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5  # Assign a default coherence score if context words are missing\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else can you tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How do the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")  # get expected response or default to empty string\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
