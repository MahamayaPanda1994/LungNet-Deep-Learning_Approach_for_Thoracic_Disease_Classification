{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: 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\n",
      "Pinecone API Key: 16db50ea-6d7a-43fe-8c8f-2e78efdd3f98\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded (optional, remove in production)\n",
    "print(\"OpenAI API Key:\", OPENAI_API_KEY)\n",
    "print(\"Pinecone API Key:\", PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 turbo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cohere\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/home/codespace/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: malformations a vms arteritis giant cell temporal takayasu’s arthritis gonococcal with inﬂammatory bowel disease juvenile rheumatoid nongonococcal acute bacterial osteoarthritis psoriatic reactive rheumatoid septic articular disease see arthritis speciﬁc types asbestosis ascariasis ascaris lumbricoides ascites aspergillosis allergic bronchopulmonary aspiration foreign body aspirin poisoning asthma astrocytoma asymptomatic bacteriuria atopic dermatitis atopic eczema atpb gene mutation atrial ﬁbrillation atrial ﬂutter atrial myxoma atrial septal defect atrial tachycardia multifocal atrioventricular block atropine as antidote poisoning with attentiondeﬁcithyperactivity disorder adhd atypical antidepressant poisoning atypical glandular cells of undetermined signiﬁcanceagus agc atypical pneumonia atypical squamous cells of undetermined signiﬁcanceascus asc austin flint murmur autoimmune disorders see rheumatologic andautoimmune disordersspeciﬁc disorders autoimmune hemolytic anemia autoimmune hepatitis avoidant skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute renal panel liver panel urinalysis hour urine ecg ■differential diagnosis •gastroenteritis •septic shock •other heavy metal toxicities including thallium iron lead andmercury •other peripheral neuropathies including guillainbarré syndrome •addison’s disease ■treatment •intravenous ﬂuids and vasopressors if necessary for hypotension •dysrhythmias lidocaine or deﬁbrillation for ventricular tachycardia intravenous magnesium or isoproterenol overdrive pacingfor torsade de pointes •benzodiazepines for seizures •chelation therapy should begin as soon as acute arsenic toxicityis suspected •if radiopaque material visible on abdominal ﬁlms bowel decontamination recommended gastric lavage followed by activated charcoalfollowed by wholebowel irrigation until abdominal ﬁlms are clear ■pearl although the point has been made repeatedly on stage and screen it isstill wise to suspect this poisoning in a widowed woman with psychiatric problems especially if it has happened more than once referencerahman mm ng jc naidu r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14747/1766213189.py:32: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  client = OpenAI()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atrial septal defect (ASD) is a congenital abnormality where there is a hole in the wall between the two upper chambers of the heart, the atria. This condition can lead to mixing of oxygen-rich and oxygen-poor blood, causing strain on the heart and lungs. Diagnosis of ASD is typically done using imaging techniques such as echocardiography, cardiac catheterization, or MRI. Treatment options may include medications, catheter-based procedures, or surgery to repair the hole in the atrial septum. It is important to monitor and manage ASD to prevent complications such as heart failure or arrhythmias."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import openai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4.0\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.0-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "# Example usage\n",
    "query = 'Hi, Sanjay. Explain on the diagnosis of Atrial Septal Defect?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for gpt-4.0:\", context)\n",
    "\n",
    "# Generating answer using gpt-4.0\n",
    "answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune model (GPT 3.5 turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving q&a pairs in .jsonl format (from .json format) \n",
    "\n",
    "import json\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a persona based chatbot. You response should be similar to the Dr.Sanjay Gupta.'\n",
    "\n",
    "def create_dataset(question, answer):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def write_jsonl(data, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for example in data:\n",
    "            example_str = json.dumps(example)\n",
    "            f.write(example_str + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = 'persona_qa_pairs.json'\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    training_data = []\n",
    "    for persona in data:\n",
    "        questions = persona[\"interview\"]\n",
    "        for qa_pair in questions:\n",
    "            prompt = qa_pair[\"question\"]\n",
    "            response = qa_pair[\"answer\"]\n",
    "            training_data.append(create_dataset(prompt, response))\n",
    "\n",
    "    jsonl_output_file = 'persona_qa_pairs.json'\n",
    "    write_jsonl(training_data, jsonl_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute refractory with excess blasts with ringed sideroblasts sickle cell sideroblastic aneurysm abdominal aortic intracranial berry angiitis allergic microscopic polyangiitis thromboangiitis obliterans angina prinzmetal’s stable unstable angina pectoris angioedema angiotensinconverting enzyme inhibitor–inducedangioedema angleclosure glaucoma index angular cheilitis candida anhedonia anomalous origin of left coronary artery anorexia anorexia nervosa anterior uveitis anthrax anticholinergic poisoning anticholinergic toxidrome antidepressant poisoning atypical tricyclic anti–glomerular basement membrane nephritis antihistamine poisoning antimuscarinic poisoning central antisocial personality disorder anxiety generalized anxiety disorder aortic coarctation aortic dissection aortic regurgitation aortic stenosis aplastic anemia apnea sleep appendicitis acute apraxia apraxia gait arnoldchiari malformation syringomyelia with arsenic poisoning arterial occlusion acute lowerextremity arteriovenous malformations a vms thrombosis dilated cardiomyopathy hypertension hypertrophic obstructive cardiomyopathy mitral regurgitation mitral stenosis multifocal atrial tachycardia myocarditis paroxysmal supraventricular tachycardia patent ductus arteriosus pericarditis acute prinzmetal’s angina pulmonary stenosis restrictive cardiomyopathy rheumatic fever acute sudden cardiac death tricuspid regurgitation tricuspid stenosis unstable angina ventricular septal defect ventricular tachycardia carpal tunnel syndrome cartilage black cataract catscratch disease celiac sprue cellulitis postseptal orbital preseptal central diabetes insipidus central nervous system tumors cerebellar hemangioblastoma cerebral vascular occlusive disease cervical cancer cervical dysplasia cervical intraepithelial neoplasia cin cervicitis mucopurulent cetuximab chagas’ disease chagoma chalazion chancroid charcot’s triad cheilitis actinic angular candida chickenpox acute chlamydia psittaci chlamydia trachomatis in epididymitis mucopurulent cervicitis from pelvic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14747/567031983.py:35: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model(messages=messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Answer: The information about yoga has been established over the past decade. The practice of yoga is associated with enhancing the quality of life. Yoga has ancient roots and is a dynamic mind-body practice that has always been embraced for its health benefits. The practice of yoga is associated with enhancing the quality of life. Yoga is a dynamic mind-body practice that has always been embraced for its health benefits. Yoga is a state of being that is associated with the union of body and mind. The practice of yoga is associated with enhancing the quality of life. Yoga is a dynamic mind-body practice that has always been embraced for its health benefits. Yoga is a state of being that is associated with the union of body and mind. Yoga has ancient roots and is a dynamic mind body practice that has always been embraced for its health benefits.\n",
      "\n",
      "The differential diagnosis of aortic stenosis includes the following conditions: acute aortic regurgitation, acute coronary syndrome, myocardial infarction, hypertrophic obstructive cardiomyopathy, and infective endocarditis. The differential diagnosis of aortic stenosis includes the following conditions: acute aortic regurgitation, acute coronary syndrome, myocardial infarction, hypertrophic obstructive cardiomyopathy, and infective endocarditis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client (model is after fine tuning)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\")  # fine-tune model\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr. Sanjay Gupta, generating responses based on the context and fine-tune information. Generate a response that is at least 300 words, sticking closely to the question.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more) for responding to the query by using words only from the context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = model(messages=messages)\n",
    "    return response.content  # Return the full content of the response\n",
    "\n",
    "# Example usage\n",
    "query = 'Hi, Sanjay. Are your answers similar to Sanjay Gupta? Then give me your stance on yoga? Finally, give me information on the differential diagnosis of Aortic Stenosis?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "answer = generate_response(context, query)\n",
    "print(\"Conversational Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import openai\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\") #after fine-tune model\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned, which generate response based on the context and the fine-tune information. Generate the response which is at least 300 words and stick mostly to the question, if there are any greetings or question on the persona then answer that part with the fine-tuning information. Remember that the responses need to be from the context mostly.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more) for responding to the query by using words only from context provided and if there are any greetings or information not related to the context only then you can use the fine-tune information or respond accordingly: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    #for chunk in response:\n",
    "        #if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = 'Hi, Sanjay. Are your answers similar to Sanjay Gupta? Then give me your stance on yoga? Finally give me information on the differential diagnosis of Aortic Stenosis?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "#print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "#answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using base model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    #for chunk in response:\n",
    "        #if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: malformations a vms arteritis giant cell temporal takayasu’s arthritis gonococcal with inﬂammatory bowel disease juvenile rheumatoid nongonococcal acute bacterial osteoarthritis psoriatic reactive rheumatoid septic articular disease see arthritis speciﬁc types asbestosis ascariasis ascaris lumbricoides ascites aspergillosis allergic bronchopulmonary aspiration foreign body aspirin poisoning asthma astrocytoma asymptomatic bacteriuria atopic dermatitis atopic eczema atpb gene mutation atrial ﬁbrillation atrial ﬂutter atrial myxoma atrial septal defect atrial tachycardia multifocal atrioventricular block atropine as antidote poisoning with attentiondeﬁcithyperactivity disorder adhd atypical antidepressant poisoning atypical glandular cells of undetermined signiﬁcanceagus agc atypical pneumonia atypical squamous cells of undetermined signiﬁcanceascus asc austin flint murmur autoimmune disorders see rheumatologic andautoimmune disordersspeciﬁc disorders autoimmune hemolytic anemia autoimmune hepatitis avoidant skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute renal panel liver panel urinalysis hour urine ecg ■differential diagnosis •gastroenteritis •septic shock •other heavy metal toxicities including thallium iron lead andmercury •other peripheral neuropathies including guillainbarré syndrome •addison’s disease ■treatment •intravenous ﬂuids and vasopressors if necessary for hypotension •dysrhythmias lidocaine or deﬁbrillation for ventricular tachycardia intravenous magnesium or isoproterenol overdrive pacingfor torsade de pointes •benzodiazepines for seizures •chelation therapy should begin as soon as acute arsenic toxicityis suspected •if radiopaque material visible on abdominal ﬁlms bowel decontamination recommended gastric lavage followed by activated charcoalfollowed by wholebowel irrigation until abdominal ﬁlms are clear ■pearl although the point has been made repeatedly on stage and screen it isstill wise to suspect this poisoning in a widowed woman with psychiatric problems especially if it has happened more than once referencerahman mm ng jc naidu r\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = 'Hi, Sanjay. Explain on the diagnosis of Atrial Septal Defect?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "#answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to get embeddings and generate response (for fine-tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get embeddings: 0.15613746643066406 seconds\n",
      "Time to generate response: 1.0686352252960205 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = get_top_3_similar(query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to get embeddings: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "answer = generate_response(context, query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate response: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to get embeddings and generate response (for base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get embeddings: 0.1641521453857422 seconds\n",
      "Time to generate response: 0.9281816482543945 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = get_top_3_similar(query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to get embeddings: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "answer = generate_response(context, query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate response: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation -- Accuracy + Coherence + Fluency (fine-tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! You can ask me anything. Type 'exit' to end the conversation.\n",
      "Bot: Yoga has been shown to have numerous health benefits, both physical and mental. It can help improve flexibility, strength, balance, and posture. Additionally, practicing yoga regularly can reduce stress, anxiety, and improve overall well-being. So yes, I would recommend incorporating yoga into your routine for better health.\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Bot: Mental health is a crucial aspect of overall well-being, encompassing a wide range of conditions such as major depressive disorder, generalized anxiety disorder, bipolar disorder, and personality disorders. It is important to recognize the signs and symptoms of mental health disorders and seek appropriate treatment. Conditions like major depressive disorder and anxiety disorders can significantly impact daily functioning and quality of life. Seeking help from mental health professionals, such as therapists or psychiatrists, can lead to effective management and improvement in mental health. Remember, taking care of your mental health is just as important as taking care of your physical health.\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Glad! I was of help\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from collections import deque\n",
    "import spacy\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "# History management class\n",
    "class LLMHistory:\n",
    "    def __init__(self):\n",
    "        self.history = []  # Store all queries and responses\n",
    "\n",
    "    def add_to_history(self, query, response):\n",
    "        \"\"\"Add a query and its response to history.\"\"\"\n",
    "        self.history.append({'query': query, 'response': response})\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Retrieve the query/response history.\"\"\"\n",
    "        return self.history\n",
    "\n",
    "# Initialize history object\n",
    "history = LLMHistory()\n",
    "\n",
    "def generate_response(context, query):\n",
    "    past_interactions = \" \".join([f\"User: {item['query']} Bot: {item['response']}\" for item in history.get_history()])\n",
    "    full_context = f\"Context: {context} Previous Interactions: {past_interactions}\" if past_interactions else context\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr. Sanjay Gupta. Generate responses based on the context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {full_context} Use this context for responding to the query: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    response_text = \"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            response_text += content\n",
    "\n",
    "    history.add_to_history(query, response_text)\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)\n",
    "    \n",
    "    if len(sentence_spans) > 0:\n",
    "        results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "    else:\n",
    "        results['fluency'] = 0\n",
    "\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5\n",
    "\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome! You can ask me anything. Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Glad! I was of help\")\n",
    "            break\n",
    "            \n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")\n",
    "        \n",
    "        metrics = evaluate_model(response, expected_response)\n",
    "        print(f\"Bot: {response}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else you can tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5700934579439252}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.4523809523809524}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.7213114754098361}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.23809523809523808}\n",
      "\n",
      "Query: How does the body organs work?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.3964757709251101}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5263157894736842}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.8484848484848485}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis, you might need to download it first using spacy download\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)  # Convert generator to list\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:  # Check if context words are available\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0  # Avoid division by zero\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5  # Assign a default coherence score if context words are missing\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")  # get expected response or default to empty string\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation -- Accuracy + Coherence + Fluency (Base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'accuracy': False, 'fluency': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data_base.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}  # Return an empty dictionary if the JSON is corrupt or empty\n",
    "    except FileNotFoundError:\n",
    "        return {}  # Return an empty dictionary if the file does not exist\n",
    "\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data_base.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None  # Placeholder for NLP-based fluency evaluation\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        save_evaluation_data(query, response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    # Loading data and evaluating a query\n",
    "    eval_data = load_evaluation_data()\n",
    "    if new_query in eval_data:\n",
    "        evaluation_results = evaluate_model(new_response, eval_data[new_query])\n",
    "        print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else can you tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.038461538461538464}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.15517241379310345}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.11711711711711711}\n",
      "\n",
      "Query: How do the body organs work?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.05263157894736842}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis, you might need to download it first using spacy download\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)  # Convert generator to list\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:  # Check if context words are available\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0  # Avoid division by zero\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5  # Assign a default coherence score if context words are missing\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else can you tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How do the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")  # get expected response or default to empty string\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/home/codespace/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! You can ask me anything. Type 'exit' to end the conversation.\n",
      "Glad! I was of help\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from collections import deque\n",
    "import spacy\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "# History management class\n",
    "class LLMHistory:\n",
    "    def __init__(self):\n",
    "        self.history = []  # Store all queries and responses\n",
    "\n",
    "    def add_to_history(self, query, response):\n",
    "        \"\"\"Add a query and its response to history.\"\"\"\n",
    "        self.history.append({'query': query, 'response': response})\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Retrieve the query/response history.\"\"\"\n",
    "        return self.history\n",
    "\n",
    "# Initialize history object\n",
    "history = LLMHistory()\n",
    "\n",
    "def generate_response(context, query):\n",
    "    past_interactions = \" \".join([f\"User: {item['query']} Bot: {item['response']}\" for item in history.get_history()])\n",
    "    full_context = f\"Context: {context} Previous Interactions: {past_interactions}\" if past_interactions else context\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr. Sanjay Gupta. Generate responses based on the context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {full_context} Use this context for responding to the query: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    response_text = \"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            response_text += content\n",
    "\n",
    "    history.add_to_history(query, response_text)\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)\n",
    "    \n",
    "    if len(sentence_spans) > 0:\n",
    "        results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "    else:\n",
    "        results['fluency'] = 0\n",
    "\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5\n",
    "\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome! You can ask me anything. Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Glad! I was of help\")\n",
    "            break\n",
    "            \n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")\n",
    "        \n",
    "        metrics = evaluate_model(response, expected_response)\n",
    "        print(f\"Bot: {response}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else can you tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5, 'bert_score': -4.925074577331543, 'perplexity': 259.9124458534727, 'query_generation_time': 3.5174896717071533}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5522388059701493, 'bert_score': 0.6932724714279175, 'perplexity': 398.818229181271, 'query_generation_time': 2.3399498462677}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.19248826291079812, 'bert_score': 0.3663818836212158, 'perplexity': 392.8128671026091, 'query_generation_time': 2.902482032775879}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.2100456621004566, 'bert_score': 0.3401765525341034, 'perplexity': 272.7881951640384, 'query_generation_time': 3.7765402793884277}\n",
      "\n",
      "Query: How do the body organs work?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.16666666666666666, 'bert_score': -0.06819698959589005, 'perplexity': 223.9203308137066, 'query_generation_time': 2.8652915954589844}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.3088235294117647, 'bert_score': 0.5095417499542236, 'perplexity': 224.4814562276125, 'query_generation_time': 1.7020103931427002}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.7105263157894737, 'bert_score': 0.5940766930580139, 'perplexity': 81.79820659885092, 'query_generation_time': 1.5681352615356445}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import time\n",
    "import numpy as np\n",
    "from bert_score import BERTScorer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize BERT scorer\n",
    "bert_scorer = BERTScorer(lang='en', rescale_with_baseline=True)\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr. Sanjay Gupta. Generate responses based on the context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context} Use this context for responding to the query: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    response = model(messages)  # Generate the response using the model\n",
    "    response_text = response.content  # Correct way to access content\n",
    "\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return {}\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None,\n",
    "        'bert_score': None,\n",
    "        'perplexity': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans) if sentence_spans else 0\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    if context_words:\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_scorer.score([response], [expected])\n",
    "    results['bert_score'] = F1.item()\n",
    "\n",
    "    # Perplexity\n",
    "    results['perplexity'] = calculate_perplexity(response)\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    # Simple perplexity calculation\n",
    "    return np.exp(np.mean([len(word) for word in text.split()]))\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else can you tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How do the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "        query_generation_time = time.time() - start_time\n",
    "        evaluation_results[query]['query_generation_time'] = query_generation_time\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (2.4.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (4.39.3)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (3.8.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from bert-score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (75.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.6.68)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->bert-score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->bert-score) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->bert-score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->bert-score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->bert-score) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->bert-score) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->bert-score) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "\u001b[33m  WARNING: The scripts bert-score and bert-score-show are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed bert-score-0.3.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
