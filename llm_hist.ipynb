{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: sk-proj-snIXVrvMjBoZgZr4WJtmT3BlbkFJ3CLhxHTD312H4OIXT9xV\n",
      "Pinecone API Key: 16db50ea-6d7a-43fe-8c8f-2e78efdd3f98\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded (optional, remove in production)\n",
    "print(\"OpenAI API Key:\", OPENAI_API_KEY)\n",
    "print(\"Pinecone API Key:\", PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 turbo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cohere\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-snIXVrvMjBoZgZr4WJtmT3BlbkFJ3CLhxHTD312H4OIXT9xV\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: malformations a vms arteritis giant cell temporal takayasu’s arthritis gonococcal with inﬂammatory bowel disease juvenile rheumatoid nongonococcal acute bacterial osteoarthritis psoriatic reactive rheumatoid septic articular disease see arthritis speciﬁc types asbestosis ascariasis ascaris lumbricoides ascites aspergillosis allergic bronchopulmonary aspiration foreign body aspirin poisoning asthma astrocytoma asymptomatic bacteriuria atopic dermatitis atopic eczema atpb gene mutation atrial ﬁbrillation atrial ﬂutter atrial myxoma atrial septal defect atrial tachycardia multifocal atrioventricular block atropine as antidote poisoning with attentiondeﬁcithyperactivity disorder adhd atypical antidepressant poisoning atypical glandular cells of undetermined signiﬁcanceagus agc atypical pneumonia atypical squamous cells of undetermined signiﬁcanceascus asc austin flint murmur autoimmune disorders see rheumatologic andautoimmune disordersspeciﬁc disorders autoimmune hemolytic anemia autoimmune hepatitis avoidant skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute renal panel liver panel urinalysis hour urine ecg ■differential diagnosis •gastroenteritis •septic shock •other heavy metal toxicities including thallium iron lead andmercury •other peripheral neuropathies including guillainbarré syndrome •addison’s disease ■treatment •intravenous ﬂuids and vasopressors if necessary for hypotension •dysrhythmias lidocaine or deﬁbrillation for ventricular tachycardia intravenous magnesium or isoproterenol overdrive pacingfor torsade de pointes •benzodiazepines for seizures •chelation therapy should begin as soon as acute arsenic toxicityis suspected •if radiopaque material visible on abdominal ﬁlms bowel decontamination recommended gastric lavage followed by activated charcoalfollowed by wholebowel irrigation until abdominal ﬁlms are clear ■pearl although the point has been made repeatedly on stage and screen it isstill wise to suspect this poisoning in a widowed woman with psychiatric problems especially if it has happened more than once referencerahman mm ng jc naidu r\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepared Context for GPT-3.5 Turbo:\u001b[39m\u001b[38;5;124m\"\u001b[39m, context)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Generating answer using GPT-3.5 Turbo\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#print(\"Conversational Answer:\", answer)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(context, query)\u001b[0m\n\u001b[1;32m     33\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     35\u001b[0m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Use this as the only context (nothing more), respond to the query by using words only from context provided: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Generate the response from the model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#prompt=prompt,\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Lower temperature for less creativity\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1268\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1256\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1264\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1265\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1266\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1267\u001b[0m     )\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:945\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1033\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1083\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1033\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1083\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1048\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1052\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1053\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1058\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import openai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "# Example usage\n",
    "query = 'Hi, Sanjay. Explain on the diagnosis of Atrial Septal Defect?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune model (GPT 3.5 turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving q&a pairs in .jsonl format (from .json format) \n",
    "\n",
    "import json\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a persona based chatbot. You response should be similar to the Dr.Sanjay Gupta.'\n",
    "\n",
    "def create_dataset(question, answer):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def write_jsonl(data, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for example in data:\n",
    "            example_str = json.dumps(example)\n",
    "            f.write(example_str + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = 'sanjay_qa_pairs.json'\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    training_data = []\n",
    "    for persona in data:\n",
    "        questions = persona[\"interview\"]\n",
    "        for qa_pair in questions:\n",
    "            prompt = qa_pair[\"question\"]\n",
    "            response = qa_pair[\"answer\"]\n",
    "            training_data.append(create_dataset(prompt, response))\n",
    "\n",
    "    jsonl_output_file = 'sanjay_qa_pairs.jsonl'\n",
    "    write_jsonl(training_data, jsonl_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute refractory with excess blasts with ringed sideroblasts sickle cell sideroblastic aneurysm abdominal aortic intracranial berry angiitis allergic microscopic polyangiitis thromboangiitis obliterans angina prinzmetal’s stable unstable angina pectoris angioedema angiotensinconverting enzyme inhibitor–inducedangioedema angleclosure glaucoma index angular cheilitis candida anhedonia anomalous origin of left coronary artery anorexia anorexia nervosa anterior uveitis anthrax anticholinergic poisoning anticholinergic toxidrome antidepressant poisoning atypical tricyclic anti–glomerular basement membrane nephritis antihistamine poisoning antimuscarinic poisoning central antisocial personality disorder anxiety generalized anxiety disorder aortic coarctation aortic dissection aortic regurgitation aortic stenosis aplastic anemia apnea sleep appendicitis acute apraxia apraxia gait arnoldchiari malformation syringomyelia with arsenic poisoning arterial occlusion acute lowerextremity arteriovenous malformations a vms thrombosis dilated cardiomyopathy hypertension hypertrophic obstructive cardiomyopathy mitral regurgitation mitral stenosis multifocal atrial tachycardia myocarditis paroxysmal supraventricular tachycardia patent ductus arteriosus pericarditis acute prinzmetal’s angina pulmonary stenosis restrictive cardiomyopathy rheumatic fever acute sudden cardiac death tricuspid regurgitation tricuspid stenosis unstable angina ventricular septal defect ventricular tachycardia carpal tunnel syndrome cartilage black cataract catscratch disease celiac sprue cellulitis postseptal orbital preseptal central diabetes insipidus central nervous system tumors cerebellar hemangioblastoma cerebral vascular occlusive disease cervical cancer cervical dysplasia cervical intraepithelial neoplasia cin cervicitis mucopurulent cetuximab chagas’ disease chagoma chalazion chancroid charcot’s triad cheilitis actinic angular candida chickenpox acute chlamydia psittaci chlamydia trachomatis in epididymitis mucopurulent cervicitis from pelvic\n",
      "Yes, my answers are similar to Sanjay Gupta. Yoga has been shown to have numerous health benefits, including reducing stress, improving flexibility, and enhancing overall well-being. When it comes to the differential diagnosis of Aortic Stenosis, it is crucial to consider other conditions that present with similar symptoms such as aortic regurgitation, hypertrophic obstructive cardiomyopathy, and mitral stenosis. Diagnostic tests like echocardiography, electrocardiogram, and cardiac catheterization can help differentiate Aortic Stenosis from these other conditions."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client (model is after fine tuning)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\") #fine-tune model\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned, which generate response based on the context and the fine-tune information. Generate the response which is at least 300 words and stick mostly to the question, if there are any greetings or question on the persona then answer that part with the fine-tuning information. Remember that the responses need to be from the context mostly.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more) for responding to the query by using words only from context provided and if there are any greetings or information not related to the context only then you can use the fine-tune information or respond accordingly: {query}\"},\n",
    "    ] #prompt\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "# Example usage\n",
    "query = 'Hi, Sanjay. Are your answers similar to Sanjay Gupta? Then give me your stance on yoga? Finally give me information on the differential diagnosis of Aortic Stenosis?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import openai\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\") #after fine-tune model\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned, which generate response based on the context and the fine-tune information. Generate the response which is at least 300 words and stick mostly to the question, if there are any greetings or question on the persona then answer that part with the fine-tuning information. Remember that the responses need to be from the context mostly.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more) for responding to the query by using words only from context provided and if there are any greetings or information not related to the context only then you can use the fine-tune information or respond accordingly: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    #for chunk in response:\n",
    "        #if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = 'Hi, Sanjay. Are your answers similar to Sanjay Gupta? Then give me your stance on yoga? Finally give me information on the differential diagnosis of Aortic Stenosis?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "#print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "#answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using base model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    # Query embedding using Cohere\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    # Querying the index for top 3 similar results\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    #for chunk in response:\n",
    "        #if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Context for GPT-3.5 Turbo: malformations a vms arteritis giant cell temporal takayasu’s arthritis gonococcal with inﬂammatory bowel disease juvenile rheumatoid nongonococcal acute bacterial osteoarthritis psoriatic reactive rheumatoid septic articular disease see arthritis speciﬁc types asbestosis ascariasis ascaris lumbricoides ascites aspergillosis allergic bronchopulmonary aspiration foreign body aspirin poisoning asthma astrocytoma asymptomatic bacteriuria atopic dermatitis atopic eczema atpb gene mutation atrial ﬁbrillation atrial ﬂutter atrial myxoma atrial septal defect atrial tachycardia multifocal atrioventricular block atropine as antidote poisoning with attentiondeﬁcithyperactivity disorder adhd atypical antidepressant poisoning atypical glandular cells of undetermined signiﬁcanceagus agc atypical pneumonia atypical squamous cells of undetermined signiﬁcanceascus asc austin flint murmur autoimmune disorders see rheumatologic andautoimmune disordersspeciﬁc disorders autoimmune hemolytic anemia autoimmune hepatitis avoidant skin vulvar squamous cells atypical of undetermined signiﬁcanceascus asc stable angina staphylococcal soft tissueskin infections staphylococcus aureus –associated toxic shock syndrome staphylococcus saprophyticus cystitis and pyelonephritisfrom stauffer’s syndrome stenosis aortic mitral pulmonary tricuspid stevensjohnson syndrome still’s disease in adults in children stones urinary streptobacillus moniliformis streptococcal pharyngitis general discussion of group a betahemolytic acute rheumatic fever from streptococcal skin infection stress disorders stress incontinence stroke hemorrhagic ischemic strongyloides stercoralis strongyloidiasis stsegment elevation myocardial infarction stemi subacute thyroiditis subarachnoid hemorrhage subconjunctival hemorrhage submandibular gland adenitis sudden cardiac death sudeck’s atrophy sugar blood high low sundowning superior vena cava syndrome from histoplasmosis supraventricular tachycardia paroxysmal surgical disorders abdominal aortic aneurysm appendicitis acute renal panel liver panel urinalysis hour urine ecg ■differential diagnosis •gastroenteritis •septic shock •other heavy metal toxicities including thallium iron lead andmercury •other peripheral neuropathies including guillainbarré syndrome •addison’s disease ■treatment •intravenous ﬂuids and vasopressors if necessary for hypotension •dysrhythmias lidocaine or deﬁbrillation for ventricular tachycardia intravenous magnesium or isoproterenol overdrive pacingfor torsade de pointes •benzodiazepines for seizures •chelation therapy should begin as soon as acute arsenic toxicityis suspected •if radiopaque material visible on abdominal ﬁlms bowel decontamination recommended gastric lavage followed by activated charcoalfollowed by wholebowel irrigation until abdominal ﬁlms are clear ■pearl although the point has been made repeatedly on stage and screen it isstill wise to suspect this poisoning in a widowed woman with psychiatric problems especially if it has happened more than once referencerahman mm ng jc naidu r\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = 'Hi, Sanjay. Explain on the diagnosis of Atrial Septal Defect?'\n",
    "result = get_top_3_similar(query)\n",
    "\n",
    "# Forming the context from results\n",
    "context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "print(\"Prepared Context for GPT-3.5 Turbo:\", context)\n",
    "\n",
    "# Generating answer using GPT-3.5 Turbo\n",
    "#answer = generate_response(context, query)\n",
    "#print(\"Conversational Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to get embeddings and generate response (for fine-tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get embeddings: 0.15613746643066406 seconds\n",
      "Time to generate response: 1.0686352252960205 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = get_top_3_similar(query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to get embeddings: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "answer = generate_response(context, query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate response: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to get embeddings and generate response (for base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get embeddings: 0.1641521453857422 seconds\n",
      "Time to generate response: 0.9281816482543945 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = get_top_3_similar(query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to get embeddings: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "answer = generate_response(context, query)\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate response: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation -- Accuracy + Coherence + Fluency (fine-tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'accuracy': False, 'fluency': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from collections import deque\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"ft:gpt-3.5-turbo-0125:personal:persona:9NuOhiSh\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "# History management class\n",
    "class LLMHistory:\n",
    "    def __init__(self, max_length=5):\n",
    "        self.history = deque(maxlen=max_length)  # Limit history to recent queries/responses\n",
    "\n",
    "    def add_to_history(self, query, response):\n",
    "        \"\"\"Add a query and its response to history.\"\"\"\n",
    "        self.history.append({'query': query, 'response': response})\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Retrieve the query/response history.\"\"\"\n",
    "        return list(self.history)\n",
    "\n",
    "# Initialize history object\n",
    "history = LLMHistory(max_length=5)\n",
    "\n",
    "def generate_response(context, query):\n",
    "    \n",
    "    client = OpenAI()\n",
    "    past_queries = \" \".join([item['query'] for item in history.get_history()])\n",
    "    full_context = f\"Context: {context} {past_queries}\" if past_queries else context\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot impersonating Dr.Sanjay Gupta as fine-tuned. Generate responses based on the context. Responses should be factual and adhere closely to the query.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {full_context} Use this context for responding to the query: {query}\"}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    response_text =  response['choices'][0]['message']['content']\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "    history.add_to_history(query, response_text)\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}  # Return an empty dictionary if the JSON is corrupt or empty\n",
    "    except FileNotFoundError:\n",
    "        return {}  # Return an empty dictionary if the file does not exist\n",
    "\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None  # Placeholder for NLP-based fluency evaluation\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        save_evaluation_data(query, response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    # Loading data and evaluating a query\n",
    "    eval_data = load_evaluation_data()\n",
    "    if new_query in eval_data:\n",
    "        evaluation_results = evaluate_model(new_response, eval_data[new_query])\n",
    "        print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else you can tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5700934579439252}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.4523809523809524}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.7213114754098361}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.23809523809523808}\n",
      "\n",
      "Query: How does the body organs work?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.3964757709251101}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5263157894736842}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.8484848484848485}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis, you might need to download it first using spacy download\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)  # Convert generator to list\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:  # Check if context words are available\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0  # Avoid division by zero\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5  # Assign a default coherence score if context words are missing\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")  # get expected response or default to empty string\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation -- Accuracy + Coherence + Fluency (Base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'accuracy': False, 'fluency': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain's ChatOpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Assuming the existence of a function to get your embedding index\n",
    "import embedding as emb\n",
    "index = emb.get_index(\"cohere-pinecone-tree\")\n",
    "\n",
    "def get_top_3_similar(query):\n",
    "    embeddings = co.embed(texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\").embeddings\n",
    "    query_results = index.query(vector=embeddings, top_k=3, include_metadata=True)\n",
    "    return query_results\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Constructing the prompt with explicit instructions\n",
    "    #prompt = f\"Using only the following context, answer the question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot which generate response solely based on the context provided. Generate the response which is at least 300 words\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context: {context} Use this as the only context (nothing more), respond to the query by using words only from context provided: {query}\"},\n",
    "    ]\n",
    "    \n",
    "    # Generate the response from the model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        #prompt=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5  # Lower temperature for less creativity\n",
    "    )\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "    return response_text\n",
    "\n",
    "def load_evaluation_data(filepath='evaluation_data_base.json'):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}  # Return an empty dictionary if the JSON is corrupt or empty\n",
    "    except FileNotFoundError:\n",
    "        return {}  # Return an empty dictionary if the file does not exist\n",
    "\n",
    "\n",
    "def save_evaluation_data(query, response, filepath='evaluation_data_base.json'):\n",
    "    data = load_evaluation_data(filepath)\n",
    "    data[query] = response\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None  # Placeholder for NLP-based fluency evaluation\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else you can tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How does the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        save_evaluation_data(query, response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    # Loading data and evaluating a query\n",
    "    eval_data = load_evaluation_data()\n",
    "    if new_query in eval_data:\n",
    "        evaluation_results = evaluate_model(new_response, eval_data[new_query])\n",
    "        print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, Sanjay. What else can you tell about healthy food?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Query: Can you explain the benefits of meditation?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.038461538461538464}\n",
      "\n",
      "Query: What are the symptoms of high blood pressure?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.15517241379310345}\n",
      "\n",
      "Query: Describe the process of photosynthesis.\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.11711711711711711}\n",
      "\n",
      "Query: How do the body organs work?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 0.5}\n",
      "\n",
      "Query: Hi, Sanjay. What is your stance on yoga?\n",
      "Metrics: {'accuracy': False, 'fluency': 1.0, 'coherence': 0.05263157894736842}\n",
      "\n",
      "Query: I have a doubt, are you really Sanjay? If so tell me a book written by you?\n",
      "Metrics: {'accuracy': True, 'fluency': 1.0, 'coherence': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis, you might need to download it first using spacy download\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy model for linguistic analysis\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def evaluate_model(response, expected):\n",
    "    results = {\n",
    "        'accuracy': response.strip() == expected.strip(),\n",
    "        'fluency': None,\n",
    "        'coherence': None\n",
    "    }\n",
    "\n",
    "    # Check fluency\n",
    "    # Check fluency\n",
    "    doc = nlp(response)\n",
    "    sentence_spans = list(doc.sents)  # Convert generator to list\n",
    "    results['fluency'] = sum([1 for sent in sentence_spans if sent.text.strip()]) / len(sentence_spans)\n",
    "\n",
    "\n",
    "    # Check coherence\n",
    "    context_words = expected.split() if expected else []\n",
    "    response_words = response.split()\n",
    "    \n",
    "    if context_words:  # Check if context words are available\n",
    "        common_words = set(context_words).intersection(set(response_words))\n",
    "        coherence_score = len(common_words) / len(context_words) if len(context_words) > 0 else 0  # Avoid division by zero\n",
    "        results['coherence'] = coherence_score\n",
    "    else:\n",
    "        results['coherence'] = 0.5  # Assign a default coherence score if context words are missing\n",
    "\n",
    "    # Adjust accuracy based on coherence score\n",
    "    if results['coherence'] >= 0.4:\n",
    "        results['accuracy'] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = [\n",
    "        'Hi, Sanjay. What else can you tell about healthy food?',\n",
    "        'Can you explain the benefits of meditation?',\n",
    "        'What are the symptoms of high blood pressure?',\n",
    "        'Describe the process of photosynthesis.',\n",
    "        'How do the body organs work?',\n",
    "        'Hi, Sanjay. What is your stance on yoga?',\n",
    "        'I have a doubt, are you really Sanjay? If so tell me a book written by you?'\n",
    "    ]\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for query in queries:\n",
    "        result = get_top_3_similar(query)\n",
    "        context = \" \".join([match['metadata']['text'] for match in result['matches']])\n",
    "        response = generate_response(context, query)\n",
    "        expected_response = load_evaluation_data().get(query, \"\")  # get expected response or default to empty string\n",
    "        evaluation_results[query] = evaluate_model(response, expected_response)\n",
    "\n",
    "    for query, metrics in evaluation_results.items():\n",
    "        print(f\"Query: {query}\\nMetrics: {metrics}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
